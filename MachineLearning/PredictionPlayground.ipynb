{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run below once to install required packages\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install seaborn\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "import sklearn.ensemble\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "#Constants\n",
    "TESTSIZE = 0.3 # x% of data will be used for testing.\n",
    "# TODO 0.3 seems to produce better results as 0.2, overfitting?\n",
    "\n",
    "#Import data\n",
    "data = pd.read_csv('../dataset/masterdataframe.csv')\n",
    "feature_cols = data.columns\n",
    "feature_firstdata = data.iloc[0]\n",
    "\n",
    "# Only use numeric data\n",
    "def filter_features():\n",
    "    valid_features = []\n",
    "    for i in range(0,len(feature_cols)):\n",
    "        if feature_cols[i] == 'result':\n",
    "            continue\n",
    "        value = feature_firstdata.values[i]\n",
    "        if value is not None and isinstance(value, float) and value is not pd.isna(value) and not math.isnan(value):\n",
    "            valid_features.append(feature_cols[i])\n",
    "    return valid_features\n",
    "\n",
    "# ['reach', 'height', 'age', 'knockdowns', 'takedowns_landed', \n",
    "#            'sig_strikes_landed', 'total_strikes_landed', 'head_strikes_landed', \n",
    "#            'body_strikes_landed', 'total_strikes_accuracy', 'head_strikes_accuracy', \n",
    "#            'height_differential', 'sub_attempts']\n",
    "\n",
    "\n",
    "#Transform date for dates of birth NOTE: DOB DECREASED ACCURACAY by 0,11%\n",
    "# data['dob'] = pd.to_datetime(data['dob'], errors='coerce')\n",
    "# data['dob'] = data['dob'].apply(lambda x: (x - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s') if pd.notna(x) else np.nan)\n",
    "# mean_dob = data['dob'].mean()\n",
    "# data['dob'] = data['dob'].fillna(mean_dob)\n",
    "\n",
    "#PREPARE DATA\n",
    "#Remove NaN from data\n",
    "df = data[data.isnull().sum(axis=1) < 1]\n",
    "filtered_features = filter_features()\n",
    "\n",
    "X = df[filtered_features]\n",
    "Y = df['result']\n",
    "\n",
    "#Test train split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=TESTSIZE, random_state=44)\n",
    "\n",
    "classifier = LGBMClassifier() #GradientBoostingClassifier() # #CatBoostClassifier() # #XGBClassifier() #GradientBoostingClassifier() #MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     #hidden_layer_sizes=(5, 2), random_state=1, max_iter=200000) # #AdaBoostClassifier() #DecisionTreeClassifier() #SVC(kernel='rbf') #SVC(kernel='linear') #LinearRegression()  #RandomForestClassifier() #KNeighborsClassifier() #DecisionTreeClassifier() #GradientBoostingClassifier() #KNeighborsClassifier() #RandomForestClassifier() #SVC(kernel='linear') #LinearRegression()\n",
    "\n",
    "\n",
    "# \n",
    "# gb_model = GradientBoostingClassifier()\n",
    "# \n",
    "# # Definie parametergrid to search in\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'max_depth': [3, 4, 5]\n",
    "# }\n",
    "# \n",
    "# grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "# \n",
    "# # Execute grid search on data\n",
    "# grid_search.fit(x_train, y_train)\n",
    "# \n",
    "# # Retrieve the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "# \n",
    "# # Use model with the best parameters\n",
    "# best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# Fit the classifier to the model\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Use the classifier to predict the outcome\n",
    "y_pred = classifier.predict(x_test) #best_gb_model.predict(x_test) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Loss', 'Win'], yticklabels=['Loss', 'Win'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Amount of features\", len(filtered_features))\n",
    "\n",
    "print(data.shape) \n",
    "#sns.pairplot(data, hue = 'result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for class imbalance\n",
    "\n",
    "# from mlxtend.plotting import plot_confusion_matrix\n",
    "# \n",
    "# \n",
    "# plot_confusion_matrix(classifier, x_test, y_test)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "# \n",
    "# \n",
    "# plt.figure(figsize=(12, 10))\n",
    "# for feature in features:\n",
    "#     plt.subplot(4, 4, features.index(feature) + 1)\n",
    "#     plt.hist(x_train[feature], bins=20, color='blue', alpha=0.7, label='Training Data')\n",
    "#     plt.hist(x_test[feature], bins=20, color='red', alpha=0.7, label='Testing Data')\n",
    "#     plt.title(f'Distribution of {feature}')\n",
    "#     plt.xlabel(feature)\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.legend()\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non allowed graph, not allowed to use test graph \n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "n_estimators_range = range(1, 101, 5)\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    train_accuracy.append(accuracy_score(y_train, clf.predict(x_train)))\n",
    "    test_accuracy.append(accuracy_score(y_test, clf.predict(x_test)))\n",
    "\n",
    "plt.plot(n_estimators_range, train_accuracy, label='Training Accuracy')\n",
    "plt.plot(n_estimators_range, test_accuracy, label='Testing Accuracy')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Testing Accuracy vs Number of Estimators')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above but attempt to plot epoch 0 at 50%, doesnt work since using test data again, see todo below.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#TODO Graph below is needed, but not allowed to use test data.\n",
    "#TODO First establish elbow curve using only training data and at that point plot test data once, single dot.\n",
    "\n",
    "#Training\n",
    "NUM_ESTIMATORS = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "for epoch in range(1, NUM_ESTIMATORS + 1):\n",
    "    classifier.set_params(n_estimators=epoch)\n",
    "\n",
    "    # Cross-validation on training data\n",
    "    train_acc = np.mean(cross_val_score(classifier, x_train, y_train, cv=5, scoring='accuracy'))\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Use one test data point for validation\n",
    "    x_test, _, y_test, _ = train_test_split(X, Y, test_size=TESTSIZE, random_state=45)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_test_pred = classifier.predict(x_test)\n",
    "    validation_acc = accuracy_score(y_test, y_test_pred)\n",
    "    validation_accuracies.append(validation_acc)\n",
    "\n",
    "# Plotting the training and validation accuracies\n",
    "epochs = np.arange(1, NUM_ESTIMATORS + 1)\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracies over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # Calculate accuracy at epoch 0\n",
    "# y_train_pred_0 = classifier.staged_predict(x_train).__next__()\n",
    "# y_test_pred_0 = classifier.staged_predict(x_test).__next__()\n",
    "# train_accuracies.append(accuracy_score(y_train, y_train_pred_0))\n",
    "# test_accuracies.append(accuracy_score(y_test, y_test_pred_0))\n",
    "# \n",
    "# # Calculate accuracy for subsequent epochs\n",
    "# for epoch, (y_train_pred, y_test_pred) in enumerate(zip(classifier.staged_predict(x_train), classifier.staged_predict(x_test)), start=1):\n",
    "#     train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "#     test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
    "# \n",
    "# # Plotting the training and test accuracies\n",
    "# epochs = np.arange(0, len(train_accuracies))  # Include epoch 0\n",
    "# plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "# plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Test Accuracies over Epochs')\n",
    "# plt.axvline(x=np.argmax(test_accuracies), color='r', linestyle='--', label='Best Test Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[data.isnull().sum(axis=1) < 1]\n",
    "\n",
    "X = df[['reach']]\n",
    "Y = df['result']\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "#for (columnName) in chosen_data:\n",
    "    #print(chosen_data[columnName])\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, Y)\n",
    "y_pred = regressor.predict(X)\n",
    "\n",
    "plt.scatter(X, Y, color = 'red')\n",
    "plt.plot(X, regressor.predict(X), color = 'blue')\n",
    "plt.title('mark1 vs mark2')\n",
    "plt.xlabel('mark1')\n",
    "plt.ylabel('mark2')\n",
    "plt.show()\n",
    "\n",
    "#print(chosen_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1. Do multiple takes of training using the various classifiers\n",
    "# TODO 2. Make a graph with the average of the outcome of those tests\n",
    "# TODO 3. Show the residuals (lines in between of average and outliers)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 1: Training data "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
