{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clf\n",
    "import np\n",
    "#Run below once to install required packages\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install seaborn\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run below once to import required packages\n",
    "import math\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSIZE = 0.3 # %X of data will be used for testing.\n",
    "VALIDSPLITSIZE = 0.1 # %X of data will be used for validation.\n",
    "\n",
    "data = pd.read_csv('../dataset/masterdataframe.csv')\n",
    "feature_cols = data.columns\n",
    "feature_firstdata = data.iloc[0]\n",
    "\n",
    "forbidden_features = [\n",
    "    'result', 'dob'\n",
    "]\n",
    "filtered_features = list(filter(lambda feature: feature not in forbidden_features and not feature.endswith('_url'), feature_cols))\n",
    "\n",
    "#Transform data for dates\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "data['date'] = data['date'].astype('int64') / 10**9\n",
    "\n",
    "#Transform time length of round (mm:ss) to seconds\n",
    "data[['minutes', 'seconds']] = data['time'].str.split(':', expand=True)\n",
    "data['time'] = pd.to_numeric(data['minutes']) * 60 + pd.to_numeric(data['seconds'])\n",
    "\n",
    "#Transform data for fighter names\n",
    "le = LabelEncoder()\n",
    "data['fighter'] = le.fit_transform(data['fighter'])\n",
    "data['opponent'] = le.fit_transform(data['opponent'])\n",
    "data['division'] = le.fit_transform(data['division'])\n",
    "data['stance'] = le.fit_transform(data['stance'])\n",
    "data['method'] = le.fit_transform(data['method'])\n",
    "data['referee'] = le.fit_transform(data['referee'])\n",
    "data['time_format'] = le.fit_transform(data['time_format'])\n",
    "\n",
    "X = data[filtered_features]\n",
    "Y = data['result']\n",
    "\n",
    "\n",
    "# First split train section\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=TESTSIZE, random_state=44)\n",
    "\n",
    "# Second split validate section\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=VALIDSPLITSIZE, random_state=44)\n",
    "\n",
    "classifier = LGBMClassifier()\n",
    "\n",
    "# Fit the classifier to the model\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Use the classifier to predict the outcome\n",
    "y_pred = classifier.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage NAN values in dob feature\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "nan_values = df[df['dob'].isna()]['dob']\n",
    "nan_percentage = (df['dob'].isna().sum() / len(df['dob'])) * 100\n",
    "\n",
    "print(\"NaN-values in column 'dob (date of birth)':\")\n",
    "print(nan_values)\n",
    "print(f\"Percentage NaN-values in column 'dob': {nan_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show importance of feature\n",
    "feat_imp = pd.Series(classifier.feature_importances_, index=X.columns)\n",
    "feat_imp.nlargest(30).plot(kind='barh', figsize=(8,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log loss plot\n",
    "\n",
    "# Parameters for LightGBM model\n",
    "NUM_ITERATIONS = 750\n",
    "LEARNING_RATE = 0.02\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_iterations': NUM_ITERATIONS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "val_data = lgb.Dataset(x_val, label=y_val, reference=train_data)\n",
    "test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Lists to store evaluation results\n",
    "evals_result_train = {'train_acc': []}\n",
    "evals_result_val = {'val_acc': []}\n",
    "evals_result_test = {'test_acc': []}\n",
    "\n",
    "# Custom callback function to collect evaluation results\n",
    "def validation_callback(env):\n",
    "    train_metric = env.evaluation_result_list[0][2]\n",
    "    val_metric = env.evaluation_result_list[1][2]\n",
    "    test_metric = env.evaluation_result_list[2][2]\n",
    "    evals_result_train['train_acc'].append(train_metric)\n",
    "    evals_result_val['val_acc'].append(val_metric)\n",
    "    evals_result_test['test_acc'].append(test_metric)\n",
    "\n",
    "# Train the model with the validation callback\n",
    "model = lgb.train(params, train_data, valid_sets=[train_data, val_data, test_data], num_boost_round=100, callbacks=[validation_callback])\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "y_pred_class = np.round(y_pred)\n",
    "\n",
    "# Accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f'Accuracy on test set: {accuracy:.4f}')\n",
    "\n",
    "# Gradient of the train accuracy curve\n",
    "grad_train_acc = np.gradient(evals_result_train['train_acc'])\n",
    "\n",
    "# Threshold percentage for determining the flat region\n",
    "threshold_percentage = 0.05\n",
    "\n",
    "# Index where the gradient is below the threshold\n",
    "cutoff_point_index = np.where(np.abs(grad_train_acc) < threshold_percentage * np.max(np.abs(grad_train_acc)))[0][0]\n",
    "\n",
    "# Accuracy over iterations\n",
    "plt.plot(evals_result_train['train_acc'], label='Train')\n",
    "plt.plot(evals_result_val['val_acc'], label='Validation')\n",
    "#plt.plot(evals_result_test['test_acc'], label='Test')\n",
    "plt.axvline(x=cutoff_point_index, color='red', linestyle='--', label='Cutoff line')\n",
    "\n",
    "# Marker for the cutoff point\n",
    "plt.scatter(cutoff_point_index, evals_result_test['test_acc'][cutoff_point_index], color='red', marker='o', label='Cutoff point (Test Set)')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Log loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data graph\n",
    "train_errors = []\n",
    "\n",
    "for epoch in range(1, 101):  # Amount of epochs\n",
    "    # Train model on trainingsset\n",
    "    classifier.fit(x_train, y_train, eval_metric='binary_error')  # Verander hier eval_metric\n",
    "\n",
    "    # Evaluete trainingsset\n",
    "    train_error = 1 - classifier.score(x_train, y_train)  # Omdat eval_metric 'binary_error' is\n",
    "    train_errors.append(train_error)\n",
    "\n",
    "# Plot training \n",
    "plt.plot(range(epoch), train_errors, label='Training Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Loss', 'Win'], yticklabels=['Loss', 'Win'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Amount of features\", len(filtered_features))\n",
    "\n",
    "print(data.shape) \n",
    "#sns.pairplot(data, hue = 'result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "\n",
    "plot_confusion_matrix(classifier, x_test, y_test)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "for feature in features:\n",
    "    plt.subplot(4, 4, features.index(feature) + 1)\n",
    "    plt.hist(x_train[feature], bins=20, color='blue', alpha=0.7, label='Training Data')\n",
    "    plt.hist(x_test[feature], bins=20, color='red', alpha=0.7, label='Testing Data')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non allowed graph, not allowed to use test graph \n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "n_estimators_range = range(1, 101, 5)\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    train_accuracy.append(accuracy_score(y_train, clf.predict(x_train)))\n",
    "    test_accuracy.append(accuracy_score(y_test, clf.predict(x_test)))\n",
    "\n",
    "plt.plot(n_estimators_range, train_accuracy, label='Training Accuracy')\n",
    "plt.plot(n_estimators_range, test_accuracy, label='Testing Accuracy')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Testing Accuracy vs Number of Estimators')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above but attempt to plot epoch 0 at 50%, doesnt work since using test data again, see todo below.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Training\n",
    "NUM_ESTIMATORS = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "for epoch in range(1, NUM_ESTIMATORS + 1):\n",
    "    classifier.set_params(n_estimators=epoch)\n",
    "\n",
    "    # Cross-validation on training data\n",
    "    train_acc = np.mean(cross_val_score(classifier, x_train, y_train, cv=5, scoring='accuracy'))\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Use one test data point for validation\n",
    "    x_test, _, y_test, _ = train_test_split(X, Y, test_size=TESTSIZE, random_state=45)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_test_pred = classifier.predict(x_test)\n",
    "    validation_acc = accuracy_score(y_test, y_test_pred)\n",
    "    validation_accuracies.append(validation_acc)\n",
    "\n",
    "# Plotting the training and validation accuracies\n",
    "epochs = np.arange(1, NUM_ESTIMATORS + 1)\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracies over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # Calculate accuracy at epoch 0\n",
    "# y_train_pred_0 = classifier.staged_predict(x_train).__next__()\n",
    "# y_test_pred_0 = classifier.staged_predict(x_test).__next__()\n",
    "# train_accuracies.append(accuracy_score(y_train, y_train_pred_0))\n",
    "# test_accuracies.append(accuracy_score(y_test, y_test_pred_0))\n",
    "# \n",
    "# # Calculate accuracy for subsequent epochs\n",
    "# for epoch, (y_train_pred, y_test_pred) in enumerate(zip(classifier.staged_predict(x_train), classifier.staged_predict(x_test)), start=1):\n",
    "#     train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "#     test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
    "# \n",
    "# # Plotting the training and test accuracies\n",
    "# epochs = np.arange(0, len(train_accuracies))  # Include epoch 0\n",
    "# plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "# plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Test Accuracies over Epochs')\n",
    "# plt.axvline(x=np.argmax(test_accuracies), color='r', linestyle='--', label='Best Test Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[data.isnull().sum(axis=1) < 1]\n",
    "\n",
    "X = df[['reach']]\n",
    "Y = df['result']\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "#for (columnName) in chosen_data:\n",
    "    #print(chosen_data[columnName])\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, Y)\n",
    "y_pred = regressor.predict(X)\n",
    "\n",
    "plt.scatter(X, Y, color = 'red')\n",
    "plt.plot(X, regressor.predict(X), color = 'blue')\n",
    "plt.title('mark1 vs mark2')\n",
    "plt.xlabel('mark1')\n",
    "plt.ylabel('mark2')\n",
    "plt.show()\n",
    "\n",
    "#print(chosen_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
